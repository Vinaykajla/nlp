{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO7Kw3e7ibBc2lIreWTRybQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**CODE 1**"],"metadata":{"id":"ooOyEYAKg08T"}},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vAGA8C4NfhM4","executionInfo":{"status":"ok","timestamp":1733061201228,"user_tz":-330,"elapsed":920,"user":{"displayName":"ES","userId":"04734404086128239854"}},"outputId":"8194ded5-4981-4e25-c2b3-47a8bf4aefe7"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["[('example', 'NN'), ('sentence', 'NN'), ('demonstrating', 'VBG'), ('part', 'NN'), ('speech', 'NN'), ('tagging', 'NN'), ('.', '.')]\n"]}],"source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","# Download required NLTK resources\n","nltk.download('punkt_tab')\n","nltk.download('averaged_perceptron_tagger_eng')\n","nltk.download('stopwords')\n","\n","# Input text\n","text = \"This is an example sentence demonstrating part of speech tagging.\"\n","\n","# Tokenize and remove stop words\n","stop_words = set(stopwords.words('english'))\n","filtered_words = [word for word in word_tokenize(text) if word.lower() not in stop_words]\n","\n","# Perform POS tagging\n","pos_tags = nltk.pos_tag(filtered_words)\n","\n","# Output\n","print(pos_tags)\n"]},{"cell_type":"markdown","source":["**CODE 2**"],"metadata":{"id":"THG605aYg58K"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Sample documents\n","documents = [\n","    \"This is a sample document.\",\n","    \"This document is another sample document.\",\n","    \"And this is yet another example document.\"\n","]\n","\n","# Initialize TF-IDF vectorizer\n","vectorizer = TfidfVectorizer()\n","\n","# Compute TF-IDF matrix\n","tfidf_matrix = vectorizer.fit_transform(documents)\n","\n","# Display TF-IDF values\n","for word, idx in vectorizer.vocabulary_.items():\n","    print(f\"{word}: {vectorizer.idf_[idx]:.3f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1tVcUlMUf3AD","executionInfo":{"status":"ok","timestamp":1733061219337,"user_tz":-330,"elapsed":343,"user":{"displayName":"ES","userId":"04734404086128239854"}},"outputId":"5ddb6d52-b624-4a58-97c9-db6ef38fc0d9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["this: 1.000\n","is: 1.000\n","sample: 1.288\n","document: 1.000\n","another: 1.288\n","and: 1.693\n","yet: 1.693\n","example: 1.693\n"]}]},{"cell_type":"markdown","source":["**CODE 3**"],"metadata":{"id":"AgL_POOVg9PZ"}},{"cell_type":"code","source":["import nltk\n","from collections import defaultdict, Counter\n","from nltk.util import ngrams\n","from nltk.tokenize import word_tokenize\n","from nltk import ConditionalFreqDist\n","\n","# Download required resources\n","nltk.download('punkt')\n","\n","class NgramModel:\n","    def __init__(self, n):\n","        self.n = n\n","        self.model = defaultdict(Counter)\n","\n","    def train(self, text):\n","        tokens = word_tokenize(text.lower())\n","        n_grams = ngrams(tokens, self.n)\n","        for gram in n_grams:\n","            prefix, next_word = tuple(gram[:-1]), gram[-1]\n","            self.model[prefix][next_word] += 1\n","\n","    def predict(self, context):\n","        context = tuple(context[-(self.n - 1):])\n","        if context in self.model:\n","            return self.model[context].most_common(1)[0][0]\n","        return None\n","\n","# Example usage\n","text = \"This is a simple example. This example is for N-gram language modeling.\"\n","model = NgramModel(n=2)  # Bigram model\n","model.train(text)\n","\n","# Predict next word\n","context = [\"this\", \"is\"]\n","print(\"Next word:\", model.predict(context))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UbLD67-Cf711","executionInfo":{"status":"ok","timestamp":1733061233472,"user_tz":-330,"elapsed":333,"user":{"displayName":"ES","userId":"04734404086128239854"}},"outputId":"47df86e7-f5a1-46a6-90ac-69da1347ca62"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Next word: a\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["**CODE 4A**\n"],"metadata":{"id":"MqaHyGwKhBUc"}},{"cell_type":"code","source":["from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","import nltk\n","\n","# Download required resources\n","nltk.download('punkt')\n","\n","# Sample corpus\n","corpus = [\n","    \"This is a simple example.\",\n","    \"Word embeddings are useful for NLP tasks.\",\n","    \"Word2Vec captures semantic relationships between words.\"\n","]\n","\n","# Preprocess: Tokenize sentences\n","tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]\n","\n","# Train Word2Vec model\n","model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n","\n","# Get vector for a word\n","word_vector = model.wv['word']  # Replace 'word' with any word in the vocabulary\n","print(f\"Vector for 'word': {word_vector}\")\n","\n","# Find most similar words\n","similar_words = model.wv.most_similar('word', topn=5)  # Replace 'word' as needed\n","print(\"Most similar words:\", similar_words)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IJndD1pjf7rV","executionInfo":{"status":"ok","timestamp":1733061256180,"user_tz":-330,"elapsed":1436,"user":{"displayName":"ES","userId":"04734404086128239854"}},"outputId":"8d54f8c6-edaa-4fe5-fe3d-0ea546224a2a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Vector for 'word': [ 8.1681199e-03 -4.4430327e-03  8.9854337e-03  8.2536647e-03\n"," -4.4352221e-03  3.0310510e-04  4.2744912e-03 -3.9263200e-03\n"," -5.5599655e-03 -6.5123225e-03 -6.7073823e-04 -2.9592158e-04\n","  4.4630850e-03 -2.4740540e-03 -1.7260908e-04  2.4618758e-03\n","  4.8675989e-03 -3.0808449e-05 -6.3394094e-03 -9.2608072e-03\n","  2.6657581e-05  6.6618943e-03  1.4660227e-03 -8.9665223e-03\n"," -7.9386048e-03  6.5519023e-03 -3.7856805e-03  6.2549924e-03\n"," -6.6810320e-03  8.4796622e-03 -6.5163244e-03  3.2880199e-03\n"," -1.0569858e-03 -6.7875278e-03 -3.2875966e-03 -1.1614120e-03\n"," -5.4709399e-03 -1.2113475e-03 -7.5633135e-03  2.6466595e-03\n","  9.0701487e-03 -2.3772502e-03 -9.7651005e-04  3.5135616e-03\n","  8.6650876e-03 -5.9218528e-03 -6.8875779e-03 -2.9329848e-03\n","  9.1476962e-03  8.6626766e-04 -8.6784009e-03 -1.4469790e-03\n","  9.4794659e-03 -7.5494875e-03 -5.3580985e-03  9.3165627e-03\n"," -8.9737261e-03  3.8259076e-03  6.6544057e-04  6.6607012e-03\n","  8.3127534e-03 -2.8507852e-03 -3.9923131e-03  8.8979173e-03\n","  2.0896459e-03  6.2489416e-03 -9.4457148e-03  9.5901238e-03\n"," -1.3483083e-03 -6.0521150e-03  2.9925345e-03 -4.5661093e-04\n","  4.7064926e-03 -2.2830211e-03 -4.1378425e-03  2.2778988e-03\n","  8.3543835e-03 -4.9956059e-03  2.6686788e-03 -7.9905549e-03\n"," -6.7733466e-03 -4.6766878e-04 -8.7677278e-03  2.7894378e-03\n","  1.5985954e-03 -2.3196924e-03  5.0037908e-03  9.7487867e-03\n","  8.4542679e-03 -1.8802249e-03  2.0581519e-03 -4.0036892e-03\n"," -8.2414057e-03  6.2779556e-03 -1.9491815e-03 -6.6620467e-04\n"," -1.7713320e-03 -4.5356657e-03  4.0617096e-03 -4.2701806e-03]\n","Most similar words: [('semantic', 0.1747603863477707), ('relationships', 0.1192832887172699), ('for', 0.11117953807115555), ('between', 0.10888773202896118), ('word2vec', 0.10561087727546692)]\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["**CODE 4B**"],"metadata":{"id":"9P0jJPz9hHar"}},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":61},"id":"-kC1ToixgdAx","executionInfo":{"status":"ok","timestamp":1733061443921,"user_tz":-330,"elapsed":44343,"user":{"displayName":"ES","userId":"04734404086128239854"}},"outputId":"161d28b4-8806-4e03-e635-3334ad4ab0e9"},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-cd7a0a12-d40a-48ed-aa14-f89634429dfe\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-cd7a0a12-d40a-48ed-aa14-f89634429dfe\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving W20.jpeg to W20.jpeg\n"]}]},{"cell_type":"code","source":["from gensim.models import KeyedVectors\n","\n","# Load GloVe pre-trained embeddings (download required .txt file first)\n","glove_path = 'glove.6B.100d.txt'  # Update path to GloVe file\n","glove_model = KeyedVectors.load_word2vec_format(glove_path, binary=False, no_header=True)\n","\n","# Get vector for a word\n","glove_vector = glove_model['word']  # Replace 'word' with your word\n","print(f\"GloVe vector for 'word': {glove_vector}\")\n"],"metadata":{"id":"CXQgoLfmgUez"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**CODE 4C**"],"metadata":{"id":"jqdN2timhKzb"}},{"cell_type":"code","source":["from gensim.models import FastText\n","\n","# Train FastText model on the tokenized corpus\n","fasttext_model = FastText(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n","\n","# Get vector for a word\n","fasttext_vector = fasttext_model.wv['word']  # Replace 'word' with your word\n","print(f\"FastText vector for 'word': {fasttext_vector}\")\n","\n","# Similarity\n","fasttext_similar = fasttext_model.wv.most_similar('word', topn=5)\n","print(\"Most similar words (FastText):\", fasttext_similar)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w9F7fjuQgDD8","executionInfo":{"status":"ok","timestamp":1733061275167,"user_tz":-330,"elapsed":2046,"user":{"displayName":"ES","userId":"04734404086128239854"}},"outputId":"8f236eab-5715-42e5-9c12-e685174e8bf6"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["FastText vector for 'word': [ 2.1158298e-03  9.8664640e-04  1.2802493e-03  2.1573605e-03\n","  1.5429289e-04 -3.0144432e-03  2.3548654e-03  9.9286779e-05\n"," -1.9067357e-03 -1.4003273e-03 -7.1609742e-04 -5.6715542e-04\n"," -6.3540565e-04 -1.6781141e-05 -4.6855113e-03  2.0611640e-03\n","  3.9697862e-03 -2.0669720e-03  1.2429424e-03  1.0267305e-03\n"," -1.2779417e-03 -4.6194455e-04 -3.1799488e-03 -3.7299070e-04\n"," -1.6909014e-03  5.3718337e-04  1.3538231e-03 -9.9462760e-04\n","  1.7661012e-03  4.2590499e-04 -3.5518100e-03 -1.7988580e-04\n"," -3.8802540e-05  4.6802667e-04 -6.3560851e-04  3.3645341e-04\n","  1.4993497e-03  1.5862887e-03 -1.9505657e-03  1.9697289e-03\n"," -1.5898142e-05 -4.0358453e-04  1.6228232e-04  1.0136354e-04\n"," -2.7229770e-03  2.1987378e-03 -1.6541592e-03  3.2168140e-03\n","  1.0731084e-03  5.4620713e-04 -3.4078488e-03 -3.7224113e-03\n"," -4.9067027e-04  1.1654234e-03  1.7591849e-03  1.4550920e-03\n"," -1.3619323e-06 -3.5131461e-04  1.3028962e-03 -3.2646924e-03\n","  6.8984582e-04  2.8526734e-04  1.9261348e-03 -2.2931029e-03\n","  1.0775318e-04  2.1369418e-03 -3.0409149e-03  1.5853178e-03\n"," -1.5922301e-04  1.3406270e-03  7.4902788e-04 -8.2168565e-04\n","  2.6605586e-03 -8.0683944e-04  1.4731051e-04 -1.1283878e-03\n","  1.6377934e-03  2.9176485e-04 -3.4441068e-03 -2.1263666e-03\n","  1.2273715e-03  1.7304397e-05  2.5931140e-03 -1.7041108e-04\n"," -6.5154553e-04 -1.4785347e-03 -1.7492571e-03  2.2953088e-04\n","  2.7133485e-03  1.7118730e-03  1.3444658e-03 -7.9062639e-04\n"," -9.1120135e-04 -9.6593518e-04 -1.0264324e-03  1.6351938e-04\n","  6.0491602e-05  1.7004833e-03 -3.1615412e-04 -1.3382462e-03]\n","Most similar words (FastText): [('words', 0.5168578624725342), ('word2vec', 0.32989147305488586), ('captures', 0.159738689661026), ('between', 0.12210491299629211), ('.', 0.08438245207071304)]\n"]}]}]}